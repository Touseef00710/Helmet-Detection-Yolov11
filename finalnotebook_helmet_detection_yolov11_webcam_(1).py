# -*- coding: utf-8 -*-
"""Finalnotebook_Helmet_Detection_Yolov11_Webcam (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dg6h64pVlp5xN1I1QA8sFygip5BE0Zsj

##  Helmet_Detection_using_Yolo11.

**Step 01**: Check GPU and Install Packages
"""

!nvidia-smi
!pip install ultralytics roboflow seaborn

"""**Step 02**: Import Required Libraries"""

import ultralytics
ultralytics.checks()

from ultralytics import YOLO
from IPython.display import Image, display
import matplotlib.pyplot as plt
import seaborn as sns
import glob, os

"""**Step 03**: Download Dataset from Roboflow"""

from roboflow import Roboflow
# Note: Replace with your actual API key if needed.
rf = Roboflow(api_key="ZkNMlNnyIa2y7w8zGWMS")
project = rf.workspace("yolo-do-it-yhopz").project("helmet-detector-9rzmg-bmd6q")
version = project.version(1)
dataset = version.download("yolov11")
print("Dataset downloaded at:", dataset.location)

"""**Step 04**: Train YOLOv11 Model"""

!yolo task=detect mode=train data={dataset.location}/data.yaml model=yolo11n.pt epochs=50 imgsz=640

"""**Step 05**: Examine Training Results"""

latest_train_run = max(glob.glob('/content/runs/detect/train*/'), key=os.path.getmtime)

display(Image(f"{latest_train_run}/confusion_matrix.png", width=600))
display(Image(f"{latest_train_run}/results.png", width=600))
display(Image(f"{latest_train_run}/train_batch0.jpg", width=600))

"""**Step 06**: Inference on Test Images"""

latest_train_run = max(glob.glob('/content/runs/detect/train*/'), key=os.path.getmtime)
best_model_path = os.path.join(latest_train_run, 'weights/best.pt')

!yolo task=detect mode=predict model={best_model_path} conf=0.25 source={dataset.location}/test/images save=True

# Find the directory with the latest prediction results
latest_predict_run = max(glob.glob('/content/runs/detect/predict*/'), key=os.path.getmtime)
for img in glob.glob(f'{latest_predict_run}/*.jpg')[:3]:
    display(Image(filename=img, width=600))

"""**Step 07**: Evaluate Model Performance"""

latest_train_run = max(glob.glob('/content/runs/detect/train*/'), key=os.path.getmtime)
best_model_path = os.path.join(latest_train_run, 'weights/best.pt')

model = YOLO(best_model_path)
metrics = model.val(data=f"{dataset.location}/data.yaml", split="test")

# Global metrics
precision = metrics.box.mp
recall = metrics.box.mr
f1 = 2 * (precision * recall) / (precision + recall + 1e-6)

print("\nüìä Evaluation Metrics:")
print(f"mAP@0.5     : {metrics.box.map50:.4f}")
print(f"mAP@0.5:0.95: {metrics.box.map:.4f}")
print(f"Precision   : {precision:.4f}")
print(f"Recall      : {recall:.4f}")
print(f"F1-score    : {f1:.4f}")

# Per-class metrics
print("\nüìå Per-Class Metrics:")
for i, cname in enumerate(model.names.values()):
    print(f"Class: {cname:12s} | "
          f"Precision: {metrics.box.p[i]:.4f} | "
          f"Recall: {metrics.box.r[i]:.4f}")

# Confusion Matrix
cm = metrics.confusion_matrix.matrix
class_names = list(model.names.values())
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt=".0f", cmap="Blues",
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix (Helmet Detection)")
plt.show()

"""# Dashboard Work

**Step1**:Install Streamlit and related packages
"""

!pip install streamlit ultralytics pyngrok pillow opencv-python matplotlib seaborn pandas

"""**Step 2**:Import pyngrok"""

from pyngrok import ngrok

"""**Step 3**: Kill any running ngrok processes"""

!pkill -f ngrok

"""**Step 4:**Set ngrok authtoken from Colab secret"""

from google.colab import userdata
import os

ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')
if ngrok_auth_token:
    ngrok.set_auth_token(ngrok_auth_token)
    print("ngrok authtoken set successfully.")
else:
    print("ngrok authtoken not found in Colab secrets. Please add it.")

"""**Step 5**:Connect to ngrok"""

public_url = ngrok.connect(8501)
print(f"Streamlit app will be available at: {public_url}")

"""**Step6**:Write the Streamlit app code to a file"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile helmet_detection_dashboard.py
# import streamlit as st
# import cv2
# import numpy as np
# import tempfile
# import time
# from PIL import Image
# import os
# import glob
# from ultralytics import YOLO
# import matplotlib.pyplot as plt
# import seaborn as sns
# import pandas as pd
# from collections import Counter
# 
# # Set page configuration
# st.set_page_config(
#     page_title="Helmet Detection Dashboard",
#     page_icon="üõ°Ô∏è",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# # Title and description
# st.title("üõ°Ô∏è Helmet Detection Dashboard")
# st.markdown("""
# This dashboard uses YOLOv11 to detect helmets in images and through webcam.
# Upload an image or use your webcam to see the detection in action!
# """)
# 
# # Sidebar
# st.sidebar.header("Settings")
# confidence_threshold = st.sidebar.slider("Confidence Threshold", 0.0, 1.0, 0.5, 0.05)
# 
# # Load model function with caching
# @st.cache_resource
# def load_model():
#     try:
#         # Find the latest trained model weights dynamically
#         latest_train_run = max(glob.glob('/content/runs/detect/train*/'), key=os.path.getmtime)
#         model_path = os.path.join(latest_train_run, 'weights/best.pt')
#         model = YOLO(model_path)
#         st.sidebar.success("Model loaded successfully!")
#         return model
#     except Exception as e:
#         st.sidebar.error(f"Error loading model: {str(e)}")
#         st.sidebar.error("No trained model found or model file is missing. Please train the model first.")
#         return None
# 
# # Load the model
# model = load_model()
# 
# # Function to process image and return results
# def process_image(image, conf_threshold=0.5):
#     if model is None:
#         return image, "Model not loaded", {}
# 
#     # Convert PIL image to numpy array (RGB format)
#     img_array = np.array(image)
# 
#     # Run inference
#     results = model(img_array, conf=conf_threshold, verbose=False)
# 
#     # Get the first result (assuming single image input)
#     result = results[0]
# 
#     # Get annotated image (which is in BGR format from result.plot())
#     annotated_img = result.plot()
# 
#     # Convert annotated image back to RGB for Streamlit
#     annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)
# 
# 
#     # Get detection statistics
#     if result.boxes is not None and len(result.boxes) > 0:
#         classes_detected = [int(cls) for cls in result.boxes.cls.tolist()]
#         confidences = result.boxes.conf.tolist()
#         class_names = [model.names[cls] for cls in classes_detected]
# 
#         stats = {
#             "total_detections": len(classes_detected),
#             "class_distribution": dict(Counter(class_names)),
#             "average_confidence": sum(confidences) / len(confidences) if confidences else 0,
#             "detections": list(zip(class_names, confidences))
#         }
#     else:
#         stats = {
#             "total_detections": 0,
#             "class_distribution": {},
#             "average_confidence": 0,
#             "detections": []
#         }
# 
#     return annotated_img_rgb, "Processing complete", stats
# 
# # Main content
# tab1, tab2, tab3 = st.tabs(["Image Upload", "Webcam", "Model Info"])
# 
# with tab1:
#     st.header("Image Upload")
#     uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
# 
#     if uploaded_file is not None:
#         # Display original image
#         image = Image.open(uploaded_file)
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.image(image, caption="Original Image", use_column_width=True)
# 
#         # Process image
#         with st.spinner("Processing image..."):
#             processed_img, status, stats = process_image(image, confidence_threshold)
# 
#         with col2:
#             st.image(processed_img, caption="Processed Image", use_column_width=True)
# 
#         # Display statistics
#         st.subheader("Detection Statistics")
#         if stats["total_detections"] > 0:
#             col1, col2, col3 = st.columns(3)
#             col1.metric("Total Detections", stats["total_detections"])
#             col2.metric("Average Confidence", f"{stats['average_confidence']:.2%}")
# 
#             # Display class distribution
#             st.write("**Class Distribution:**")
#             for class_name, count in stats["class_distribution"].items():
#                 st.write(f"- {class_name}: {count}")
# 
#             # Display detections with confidence
#             st.write("**Detections with Confidence:**")
#             for class_name, confidence in stats["detections"]:
#                 st.write(f"- {class_name}: {confidence:.2%}")
#         else:
#             st.warning("No objects detected. Try lowering the confidence threshold.")
# 
# with tab2:
#     st.header("Webcam Capture")
# 
#     # Webcam capture using st.camera_input
#     webcam_image = st.camera_input("Take a picture with your webcam")
# 
#     if webcam_image is not None:
#         # Display original image
#         image = Image.open(webcam_image)
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.image(image, caption="Webcam Capture", use_column_width=True)
# 
#         # Process image
#         with st.spinner("Processing webcam image..."):
#             processed_img, status, stats = process_image(image, confidence_threshold)
# 
#         with col2:
#             st.image(processed_img, caption="Processed Image", use_column_width=True)
# 
#         # Display statistics
#         st.subheader("Detection Statistics")
#         if stats["total_detections"] > 0:
#             col1, col2, col3 = st.columns(3)
#             col1.metric("Total Detections", stats["total_detections"])
#             col2.metric("Average Confidence", f"{stats['average_confidence']:.2%}")
# 
#             # Display class distribution
#             st.write("**Class Distribution:**")
#             for class_name, count in stats["class_distribution"].items():
#                 st.write(f"- {class_name}: {count}")
#         else:
#             st.warning("No objects detected. Try lowering the confidence threshold.")
# 
# with tab3:
#     st.header("Model Information")
# 
#     if model is not None:
#         st.subheader("Model Architecture")
#         st.write(f"Model type: YOLOv11")
#         st.write(f"Number of classes: {len(model.names)}")
# 
#         st.subheader("Class Names")
#         for i, class_name in model.names.items():
#             st.write(f"{i}: {class_name}")
# 
#         # Try to load training results if available
#         try:
#             # Find the latest training results file dynamically
#             results_path = glob.glob('/content/runs/detect/*/results.csv')[0]
#             results_df = pd.read_csv(results_path)
# 
#             st.subheader("Training Metrics")
# 
#             # Plot metrics
#             fig, axes = plt.subplots(2, 2, figsize=(12, 8))
# 
#             # mAP50
#             axes[0, 0].plot(results_df['metrics/mAP50(B)'])
#             axes[0, 0].set_title('mAP50')
#             axes[0, 0].set_xlabel('Epoch')
#             axes[0, 0].set_ylabel('Score')
# 
#             # Precision
#             axes[0, 1].plot(results_df['metrics/precision(B)'])
#             axes[0, 1].set_title('Precision')
#             axes[0, 1].set_xlabel('Epoch')
#             axes[0, 1].set_ylabel('Score')
# 
#             # Recall
#             axes[1, 0].plot(results_df['metrics/recall(B)'])
#             axes[1, 0].set_title('Recall')
#             axes[1, 0].set_xlabel('Epoch')
#             axes[1, 0].set_ylabel('Score')
# 
#             # Loss
#             axes[1, 1].plot(results_df['train/box_loss'], label='Box Loss')
#             axes[1, 1].plot(results_df['train/cls_loss'], label='Class Loss')
#             axes[1, 1].set_title('Training Loss')
#             axes[1, 1].set_xlabel('Epoch')
#             axes[1, 1].set_ylabel('Loss')
#             axes[1, 1].legend()
# 
#             plt.tight_layout()
#             st.pyplot(fig)
# 
#         except (IndexError, FileNotFoundError):
#             st.info("Training results not found. Train the model first to see metrics.")
#     else:
#         st.error("Model not loaded. Please check if the model file exists.")
# 
# # Footer
# st.markdown("---")
# st.markdown("### üõ°Ô∏è Helmet Detection Dashboard | Built with YOLOv11 and Streamlit")

""" **Step7:**Run the Streamlit app"""

!streamlit run helmet_detection_dashboard.py --server.port 8501 --server.address 0.0.0.0 > /dev/null 2>&1 &

"""**Step 8**:Check if model exists"""

import glob
import os

model_paths = glob.glob('/content/runs/detect/*/weights/best.pt')
if model_paths:
    print(f"‚úÖ Model found at: {model_paths[0]}")
    print("The Streamlit app should work correctly.")
else:
    print("‚ùå No trained model found.")
    print("Please train the model first by running the training cells.")
    print("After training, run the Streamlit app again.")